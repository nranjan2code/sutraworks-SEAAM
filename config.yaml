# SEAA Configuration
# This file configures the self-evolving system

version: "1.0.0"
environment: development

llm:
  provider: ollama
  model: qwen2.5-coder:14b
  temperature: 0.1
  max_retries: 3
  timeout_seconds: 120
  ollama_url: http://localhost:11434/api/generate
  gemini_model: gemini-1.5-flash

paths:
  dna: ./dna.json
  soma: ./soma


metabolism:
  cycle_interval_seconds: 30
  max_organs_per_cycle: 3
  max_concurrent_organs: 20
  max_total_organs: 50

circuit_breaker:
  max_attempts: 3
  cooldown_minutes: 30

genealogy:
  enabled: true
  user_name: "SEAA Genesis"
  user_email: "genesis@seaa.internal"

event_bus:
  # Number of events to retain for debugging
  # 0 = no retention (streaming only)
  # > 0 = keep last N events in memory
  max_retained_events: 100

security:
  # IMPORTANT: Disabled by default for security
  # Set to true only if you trust the LLM output
  allow_pip_install: false
  allowed_pip_packages:
    - watchdog
    - streamlit
    - flask
    - fastapi
    - requests

logging:
  level: DEBUG
  format: colored  # 'colored' for dev, 'json' for production
  # file: ./logs/seaa.log  # Uncomment to enable file logging

remote_logging:
  # Remote logging configuration (for centralized monitoring)
  enabled: false
  # URL to send logs to (must support POST with JSON)
  # Example: "http://logging-service:5000/logs" or "https://example.com/api/logs"
  url: ""
  # API key (optional, added as "Authorization: Bearer {api_key}" header)
  api_key: ""
  # Buffer size before sending batch
  batch_size: 50
  # Send interval in seconds
  flush_interval_seconds: 10
  # Security: Only send these log levels
  min_level: "WARNING"  # Only WARNING and ERROR
